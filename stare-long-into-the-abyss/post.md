---
title: "Stare Long Into The Abyss"
---

_Written on 2022-03-06_

### The engineer mindset
There seems to be a certain peculiar "engineer way of thinking", and every nontechnical counterpart I've ever worked with has been mildly frustrated by it at least once. The actual components of the mindset are hard to put into words, but they seem to be something like:

* Tendency to focus on details & implementation
* Correctness is important
* Tendency to focus on edge cases
* Rules-based mindset
* Discomfort with ambiguity
* Underemphasis of emotions
* Preference for system needs rather than user needs

Dilbert represents a great hyperbole of the software engineer's perspective: management is dead weight, other engineering coworkers are sympathetic, and non-engineering coworkers are a mixed bag with a skew towards the negative. Why does the stereotypical engineer view the world this way, and why is this mindset common regardless of whether the engineer is doing software, construction, aerospace, etc.? 

Nietzsche says, "if you stare long into the abyss, the abyss stares back at you". I hypothesize that, by the nature of engineering work, an engineer is forced to adapt their pattern of thinking to be successful. Every engineer works with some System, and these are the System characteristics that I believe shape the mind of the engineer:

1. The System is persistent
1. The System can't lie
1. You engineer can't lie to the System

### The System is persistent
I speculate that all work falls al
Nearly everything an engineer builds is intended to last. Even prototypes & MVPs are intended to be transformed into 

### The engineer's system is simple
We know that reality doesn't lie. A shoddily-constructed building falls down, poorly-written code doesn't work or is riddled with bugs, an error in a kinematics equation means missing the target, and a bad rocket design results in a wrecked rocket. There are no "maybe"s and "might"s for the engineer because the output is binary: either the system functions as expected or it doesn't. The engineer's mind therefore becomes adapted to understanding these rules because it's impossible to lie in the spheres the engineer operates in. Put another way, we could say that the engineer's effort (input) is _tightly-coupled with output_: if the engineer designs the right computer code then the machine will do what's expected, and if the engineer designs the right building plan then the building will last for decades. This gives the engineer strong predictive power for the results of their actions.

Contrast this with fields where effort is _loosely-coupled with output_. Politicians and CEOs and managers and generals are forced to execute their ideas through chains of intermediaries, which makes the system they're operating on more complex. More complexity means more moving parts means more opportunity for [emergent phenomena](https://en.wikipedia.org/wiki/Emergence) means more uncertainty about which inputs work and which don't. In large enough systems like national governments and financial markets, [it might even be impossible to predict the outcomes of actions](https://www.goodreads.com/book/show/38315.Fooled_by_Randomness). Small wonder that, from the engineer's perspective, management and politicians are largely useless: the engineer's mind has been trained on simple systems with clear cause-and-effect, so operators in uncertain, complex systems aren't valued (though it doesn't help that the uncertainty in complex systems allows the likes of politicians to claim credit for the good outcomes while divorcing themselves from bad outcomes).

I'd therefore hypothesize that the engineer tendencies for correctness, detail focus, and unambiguity are a natural consequence of the simple systems found in engineering. If the engineer doesn't think of the edge cases, the system won't work (sometimes catastrophically) and the engineer will feel the pain. If the engineer plugs in the correct inputs and accounts for the edge cases, the system will work and the engineer is rewarded.

### The engineer's system is well-mapped
Because cause and effect is so easily determined in the engineering spheres and because the success criteria is so clear, the good inputs that produce the engineer's desired output are usually well-known. Humanity has millions of words written about the right beams to use to support such-and-such weight, or the right code to download a webpage, or the right reagents to produce a given compound (and every engineer knows the fear and excitement that accompanies needing to solve a problem with no existing solution). By contrast, every manager knows the experience of trying to apply an idea and having it backfire in his or her face because the people involved are entirely different.

Put another way, engineers have many best practices that are broadly applicable because all engineers have the same "did it work?" success criteria and are operating in the same relatively simple environments. Rigid, rules-based decision trees make a lot of sense in these environments because the ground is quite well-trodden and deviations are generally unwise. Naturally, the engineer's mind adapts to think in deterministic "if/then" chains, and sources of chaos (e.g. emotions) are uncomfortable.

### The System is persistent
We created an axis between simple systems and complex systems defined by "How clear is the relationship between cause & effect?" and located engineers near the lefthand side. I'd postulate a second axis defined by "How long are you forced to deal with the consequences of your actions?", where the left side is "never" and the right is "forever". Here, engineers and management actually group together on the right: the systems that these professions interact with are quite persistent, so cheap shortcuts will come back to bite them eventually.



[the prisoner's dilemma is iterated](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma#The_iterated_prisoner's_dilemma) and cheap shortcuts are discouraged.




<!-- TODO something about how to judge engineers vs CEOs???? -->
* Scientists: simple, unmapped systems
* Politicians: complex, unmapped systems
* CEOs: complex, partially mapped systems




spans "the system is transient" to "the system is persistent". In a phrase, this axis can be encapsulated as, "How much are you forced On this axis, the left side contains realtors, consultants, 
l






<!-- TODO graph of the axes??? -->




of the principle of "garbage in, garbage out": those trainees who 



There are clear rules governing the spheres that engineers operate in, and an engineer who makes a mistake



What's less obvious is how tightly-linked an engineer's effort is to 


. If the system that the engineer is building is a function, then on i


The more direct interaction with reality one has, the easier it is to see if one's ideas are functional. 

However, as one moves farther from direct interaction with reality, cause-and-effect is more difficult to see: heads of large organizations execute their ideas through chains of intermediaries, meaning more chances for [emergent phenomenon](https://en.wikipedia.org/wiki/Emergence) to appear, meaning more uncertainty about which ideas work and which don't.

If all jobs fall somewhere on the spectrum of "long feedback loop between cause & effect" to "short feedback loop between cause & effect", engineers are near the rightmost end while politicians and consultants and CEOs are near the leftmost. There are no "maybe"s and "might"s for the engineer because the output is binary: either the system functions as expected or it doesn't. Contrast this with a CEO implementing a new policy, where the desired outcome of the policy may or may not happen contingent upon a thousand external factors (and a mistake in policy implementation may well result in a better outcome!).

It's no wonder that the engineer's mind becomes extremely pragmatic: if the engineer makes a mistake, the system will reflect the mistake in failure.

### The engineer can't lie to the System
Likewise, the inputs that an engineer feeds into the system - the computer code, the beams and trusses of a building, the shape of an airplane wing - must be precise. Unlike a lawyer who might not need to handle every single edge case, a 


or it doesn'tEngineers need to do things that demonstrably functions Right Now, so the engineer's mind becomes optimized to answer a single question: does it work? There are no "maybe"s and "might"s because the output is binary: either the system functions as expected or it doesn't.





It's no wonder that many engineers view politicians and middle management as useless (though I'd argue the average engineer doesn't fully appreciate the difficulty of working through indirect interaction).

Thus, the engineer's mind becomes :w



The engineer is forced to reckon with not designing the system well, and it's small 






, meaning more plausible deniability to say "The good things were mine and the bad things were back luck."




Small wonder indeed that engineers like Dilbert consider middle management to be useless: the engineer is used to short, pragmatic feedback cycles, and is frustrated by the perceived lack of results from the management.

Therefore, to be 








Let's imagine all jobs fall somewhere on the spectrum of "long feedback loop between action & effect" to "short feedback loop between action & effect". Engineering is near the right end of the spectrum: a shoddily-constructed building falls down, poorly-written code doesn't work or is riddled with bugs, and bad rocket design results in a wrecked rocket. Politics or middle management, by contrast, are near the left: actions have a very long lag time to result, so it's much more difficult to . That's _not_ to say that the latter jobs are _useless_

With all engineering, the system that the engineer builds either works or it doesn't


, where one can be extremely successful without actually doing something an engineer would consider useful (though I'll hasten to add that this _doesn't_ mean those jobs are useless - simply that 

### You can't lie to the System

### The System is persistent


<!-- TODO SYNTHESIZE -->
Codie's quote about how he learned not to hide problems, adn 

This mindset doesn't seem unique to software engineering (though Dilbert encapsulates everything 
